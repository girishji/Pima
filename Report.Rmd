---
title: "Forecasting the Onset of Diabetes Milletus"
subtitle: "An application of binary choice models"
author: "Girish Palya (425998) and Karina Norvoish ()"
date: "`r Sys.Date()`"
output:
  tufte::tufte_html: default
  tufte::tufte_handout:
    citation_package: natbib
    latex_engine: xelatex
  tufte::tufte_book:
    citation_package: natbib
    latex_engine: xelatex
bibliography: skeleton.bib
link-citations: yes
---

```{r setup, include=FALSE}
library(tufte)
library(tidyverse)
library(knitr)
library(GGally)
library(kableExtra)
# invalidate cache when the tufte version changes
knitr::opts_chunk$set(tidy = FALSE, cache.extra = packageVersion('tufte'))
options(htmltools.dir.version = FALSE)
```

# Abstract

Statistical methods have been used to help medical professionals make better diagnosis.
In this study we apply binomial choice models like logit and probit
to a data set from a population at high risk of the onset of diabetes. 
We analyze the performance of models using standard measures
for clinical tests: sensitivity, specificity, and a receiver operating
characteristic curve. A description of the models
is included.

# Introduction

[Diabetes mellitus](https://en.wikipedia.org/wiki/Diabetes), commonly 
known as diabetes, is a group of metabolic disorders characterized by 
a high blood sugar level over a prolonged period of time. It afflicts 
nearly 9% of world population (463 million) and causes 4 million 
deaths every year.

The ability to forecast is central to many medical situations. Standard statistical 
techniques such as discriminate
analysis, regression analysis, and factor analysis have been used to
provide this ability. Even though sophesticated neural network models have been
used
to predict the onset of diabetes using the same data set, we beleive
there is merit in using workhorse statistical models since sample size is
fairly large (768 observations) and underlying functional correlations
among variables can be evaluated and minimized. 

# Related Work

Smith et. al.^[JW Smith, JE Everhart, WC Dicksont,
WC Knowler, RS Johannes. 1988. *Using the ADAP Learning Algorithm to Forecast
the Onset of Diabetes Mellitus*] have used the ADAP Learning Algorithm to forecast
the onset of diabetes mellitus using the same data. 
They describe ADAP as "an adaptive learning routine that generates and executes
digital analogs of perceptron-like devices". 

> "ADAP was developed by two of the authors [Smith, Dickson] in 1961. We chose to examine the ADAP algorithm and test its use in forecasting the onset of non-insulin-dependent diabetes mellitus (DM) within a five-year period. The data used in this study were from the Pima Indian population near Phoenix, Arizona. Once the algorithm had been trained using 576 cases, ADAP was used to forecast whether another 192 test cases would develop diabetes within five years. Forcing ADAP to conclude on all test cases produced a sensitivity and specificity of 76 percent."


In diagnostic medicine, testing the hypothesis that the ROC Curve area or partial 
area has a specific value is a common practice^[*Statistical Methods in 
Diagnostic Medicine*, Second Edition, Ch.4,
Author(s): Xiao‐Hua Zhou Nancy A. Obuchowski Donna K. McClish]. 

Although many sophisticated models have been developed for discriminant analysis, 
recent empirical comparisons indicate that standard methods such as 
logistic regression work very well^[C B Begg. *Statistical Methods in Medical Diagnosis*].

Our null hypothesis is that binary choice models like logit and probit
will produce as good a result as ADAP.

*H0: Crossover point for sensitivity and specificity for binary choice models is greater than 0.76.*

# Data

Data comes from [Kaggle](https://www.kaggle.com/uciml/pima-indians-diabetes-database) via UCI Machine Learning Repository.

`r margin_note('A subset of data.')`
```{r echo=T}
diabetes <- read.csv(
  "https://raw.githubusercontent.com/girishji/Pima/master/data/diabetes.csv")
diabetes[1:6, ] %>% kable()
```

## Study Population

The population for this study was the Pima Indian population near
Phoenix, Arizona. That population has been under continuous
study since 1965 by the 
[National Institute of Diabetes and Digestive and Kidney Diseases](https://www.niddk.nih.gov/) 
because of its high incidence rate
of diabetes. Each community resident over 5 years of age
was asked to undergo a standardized examination every two years,
which included an oral glucose tolerance test. Diabetes was
diagnosed according to 
World Health Organization Criteria^[World Health Organization, *Report of a Study Group: Diabetes Mellitus*. World Health Organization Technical
Report Series. Geneva, 727, 1985.];
that is, if the 2 hour post-load plasma glucose was at least 200
mg/dl (11.1 mmol/l) at any survey examination or if the 
[Indian Health Service Hospital](https://www.ihs.gov/) serving the 
community found a glucose
concentration of at least 200 mg/dl during the course of routine 
medical care^[Knowler, W.C., P.H. Bennett, RF. Hamman, and M.
Milier. 1978. *Diabetes incidence and prevalence in Pima
Indians: a 19-fold greater incidence than in Rochester*,
Minnesota. Am J Epidemiol 108:497-505.]. 
In addition to being a familiar database to the
investigators, this data set provided a well validated data resource
in which to explore prediction of the date of onset of diabetes in a
longitudinal manner.

## Variables

The subjects of the study are Pima Indian women over 21 years of age. 
The following explanatory variables are found to be the risk factors 
for diabetes among women.

1. `Pregnancies`: Number of times pregnant
2. `Glucose`: Plasma Glucose Concentration at 2 Hours in an Oral
Glucose Tolerance Test (GTIT) (in mg/dL)
3. `BloodPressure`: Diastolic Blood Pressure (mm Hg)
4. `SkinThickness`: Triceps Skin Fold Thickness (mm)
5. `Insulin`: 2-Hour Serum Insulin (Uh/ml)
6. `BMI`: Body Mass Index (Weight in kg / (Height in in))
7. `DPF`^[Diabetes Pedigree Function was developed 
by Smith et. al.* to provide a measure of the expected 
genetic influence of affected and unaffected relatives on the 
subject's eventual diabetes
risk. It  uses information from parents, grandparents,
full and half siblings, full and half aunts and uncles, and first
cousins.]: Diabetes Pedigree Function 
`r margin_note("* JW Smith, JE Everhart, WC Dicksont, WC Knowler, RS Johannes. 1988. *Using the ADAP Learning Algorithm to Forecast the Onset of Diabetes Mellitus*")`
8. `Age`: Age (years)

The binary dependent variable (`Outcome`) describes 
the onset of non-insulin-dependent diabetes
mellitus (DM) within a five-year period. This variable is 1 if diagnosis 
is positive within five years, and 0 otherwise.

## Observations from Data

Preliminary examination of data reveals following:

- There are many spurious 0 values in data, especially in `Insulin` and
`SkinThickness`. These values skew the mean and cause excessive outliers. 
We assume these to be errors in data, and suitable remedies
will be employed to address the skew.

- There are 500 negative instances (65.1%)
of the regressand (`Outcome`), comapared to 258 positive instances (34.9%).
Since CDFs underlying binomial logit and probit
models differ most in the tails, and produce similar predicted probabilities
for non-extreme values^[Christopher Baum. *An Introduction to Modern Econometrics Using Stata*], it is likely that the results obtained from logit and probit models 
are going to be similar.

- From the density plot (after temporarily filtering out spurious zero 
values) we can observe that not all distributions are normal. Data is skewed
towards younger subjects. `Pregnancies` are also skewed towards fewer pregnancies.
`BMI`, `Insulin`, and `SkinThickness` have long tails on the right side.



```{r echo=F}
diabetes %>%
  summarise_each(~ sum(.x == 0)) %>%
  pivot_longer(everything(), names_to = 'Variable',
               values_to = 'Count of Zero Values') %>%
  kable(caption = 'Zero values in some variables indicate errors in data.') %>% 
  kable_styling(full_width = F)
```

$\\$

```{r fig.cap = "Box plot showing spurious outliers in `Insulin` and `SkinThickness`, and skewed mean (red dot).", cache=TRUE, echo = F}
diabetes %>% 
  select(-Outcome) %>% 
  pivot_longer(everything(), names_to = 'Variable', 
               values_to = 'Value') %>% 
  ggplot(aes(x = Variable, y = Value, fill = Variable)) +
  geom_jitter(size=0.1, alpha=0.2) +
  stat_summary(fun=mean, geom="point", shape=20, size=4, color="red", fill="red") +
  geom_boxplot(alpha = 0.3) +
  theme(legend.position="none") +
  xlab("") +
  ylab('')
```


```{r fig.cap = "Density plot of regressors (without spurious zero values).", cache=TRUE, echo=F, fig.height=3}
diabetes %>% 
  select(-Outcome) %>% 
  pivot_longer(everything(), names_to='Regressor', 
               values_to='Value') %>% 
  # Filter out spurious zeroes
  filter(Regressor == 'Pregnancies' | Value != 0) %>% 
  ggplot(aes(x = Value, group = Regressor, 
             fill = Regressor)) +
  geom_density(alpha = 0.3) +
  facet_wrap(~Regressor, scales = "free", nrow = 2) +
  theme(legend.position="none") +
  xlab('') +
  ylab('')
```




```{r fig.cap = "Correlation among regressors.", cache=TRUE, fig.margin = TRUE, echo = F}
diabetes %>% 
  rename(Pregnan. = 1, BloodPr. = 3, SkinThick. = 4) %>% 
  ggcorr(method = c("everything", "pearson"))
```

Scatter plots and correlation matrix reveal the following:

- Among all the regressors, `Glucose` concentration has the highest correlation (.467) with onset of diabetes.

- `Age` is not strongly correlated (.238) with the onset of diabetes. This seems to suggest that, either accumulation of unhealthy lifestyle habits is not a factor, or that such factors have already expressed themselves by the age of 21 years.

- Diabetes Pedigree Function (`DPF`) is not highly correlated with onset of diabetes, which suggests that hereditory factors may be less important.

- `Insulin` and `SkinThickness` show significant correlation (.437), but this maybe a result of both variables having too many spurios zero values.

- Presence of spurious zero values in `Insulin` could be masking its correlation with `Glucose`.

- None of the explanatory variables are strongly correlated with the response variable.

```{r fig.cap = "Correlation matrix of regressors.", cache=TRUE, echo=FALSE}
diabetes %>% 
  ggpairs(., lower = list(continuous = 
                            wrap("points", alpha = 0.3, 
                                 size = 0.1)))
```

# Method


# Model

```{r include=F}
library(arm)
```

Logistic regression is the standard way to model binary outcomes. We model the probability that $y = 1$,
$$P(y_i = 1) = logit^{−1}(X_iβ)$$
under the assumption that the outcomes $y_i$ are independent given these probabilities.
We refer to $Xβ$ as the linear predictor. The function $logit^{-1}(x) = \frac{e^x}{1+e^x}$ transforms 
the continuous values to the range (0, 1), which is necessary since probabilities must be between 0 and 1.

In building our regression model we include all input variables that are expected to play a 
significant role in predicting the outcome, and also include their interactions if they have
large effects. We include statistically non-significant variables as long as they have
expected sign, and exclude them if they do not have expected sign.

We start with a simple model and then build in additional complexity, taking care to check 
for problems along the way. We follow the strategy as the one used by
Gelman and Hill^[p.69, Andrew Gelman and 
Jennifer Hill, Data Analysis Using Regression and Multilevel/Hierarchical Models, 2006.].

*Logistic regression with just one predictor*

Glucose level been known to be a strong predictor of onset of diabetes, and the correlation
matrix reflects this relationship. We shall first fit the model just using `Glucose` and then
put in other variables.


```{r}
fit.1 <- glm(Outcome ~ Glucose, diabetes,
             family = binomial(link = "logit")) 
display(fit.1, digits = 4)
```

The coefficient for `Glucose` is .0379, which seems low, but it is measured in mg/dL and mean 
value is 120 mg/dL. It is possible to rescale values (say, divide by 10) to increase this coefficient,
but we chose to retain 
the original scale.

*Graphing the fitted model*

```{marginfigure}
Graphical expression of the fitted logistic regression, $$P(diabetes) = logit^{−1}(-5.3501 + 0.0379 · Glucose)$$. The predictor `Glucose`: blood sugar levels in mg/dL.
```

```{r warning=F, message=F}
diabetes %>% mutate(Predicted = fitted(fit.1)) %>% filter(Glucose != 0) %>% 
  ggplot(aes(Glucose, Outcome)) + geom_point() +
  stat_smooth(aes(Glucose, Predicted), se = F) +
  ylab('Probability (diabetes)')
```

```{r echo=F, message=F, fig.margin=T, fig.cap="The solid line shows the best-fit logistic regression, and the light lines show uncertainty in the fit."}
sim.1 <- sim(fit.1)
plotx <- function(.data) {
  plt <- .data %>% mutate(Predicted = fitted(fit.1)) %>% filter(Glucose != 0)
  for (j in 1:10) {
    plt <- plt %>% mutate(!!sym(str_c('s_', j)) := 
                            invlogit(sim.1@coef[j,1] + sim.1@coef[j,2] * Glucose))
  }
  plt <- plt %>% ggplot(aes(Glucose, Outcome)) + geom_point() 
  for (j in 1:10) {
    plt <- plt + stat_smooth(aes(Glucose, !!sym(str_c('s_', j))), se = F, color = "gray")
  }
  plt <- plt + 
    stat_smooth(aes(Glucose, Predicted), se = F) +
    labs(y = "Probability (diabetes)", title = "")
  return(plt)
}
plotx(diabetes) 
```

*Interpreting the logistic regression coefficients*

Our model is $$P(diabetes) = logit^{−1}(-5.3501 + 0.0379 · Glucose)$$

- The constant term can be interpreted when `Glucose` = 0, in which case the probability of 
diagnosing diabetes is $logit^{−1}(-5.3501) = $ `r round(invlogit(-5.3501), digits=4)` 
(close to 0%). However `Glucose` level at 0 does not make sense, so we try not
to interet this.

- We can evaluate the predictive difference with respect to `Glucose` by computing the 
derivative at the average value of `Glucose` in the dataset, which is 120.9 mg/dL. The 
value of the linear predictor here is -5.3501 + 0.0379 · 120.9 = -0.76799, and so 
the slope of the curve at this point is $0.0379e^{-0.76799}/(1+e^{-0.76799})^2$ = .0082. 
Thus, adding 1 mg/dL to `Glucose` corresponds to a positive difference in the probability of
diagnosing diabetes by about 0.82%.

- We can also look at the statistical significance of the coefficient for `Glucose`. The 
slope is estimated well, with a standard error of only 0.0033, which is tiny compared 
to the coefficient estimate of 0.0379. The approximate 95% (2 standard errors) interval 
is [0.0313, 0.0445], 
which is clearly statistically significantly different from zero. 

*Adding a second input variable*

We extend the model by adding BMI. We expect the coefficient to be positive.

```{r}
fit.2 <- glm(Outcome ~ Glucose + BMI, diabetes,
             family = binomial(link="logit"))
display(fit.2, digits = 4)
```

Comparing two individuals, the one with 1 mg/dL higher blood glucose will encounter
0.0352 logit probability of diabetes diagnosis. Similarly, an increasea of 1 kg/in in
BMI corresponds to an increase of 0.0763 logit prbability of diagnosis. Both 
coefficients are statistically significant, each being more than 2 standard errors 
away from zero. And both their signs make sense: glucose level and BMI are known
risk factors for diabetes.

For a quick interpretation, we use the “divide by 4 rule”^[p.82, Andrew Gelman and 
Jennifer Hill, Data Analysis Using Regression and Multilevel/Hierarchical Models, 2006.]
on the coefficients and observe that 1 mg/dL increase in glucose increases probability
of diabetes diagnosis by 0.88%, and 1 kg/in increase in BMI also increases probability
of diabetes diagnosis by 1.9%.

Comparing these two coefficients, we may conclude that `BMI` is more a important factor, 
but this is incorrect. The standard deviation of `BMI` is 7.9 and for `Glucose` it is 31.9.
The logistic regression coefficients corresponding to 1-standard-deviation differences 
are 0.0352·31.9 for `Glucose` and  0.0763·7.9 for `BMI` respectively. Again, applying
the "divide by 4 rule", 1-standard deviation difference of `Glucose` yields a 28%
increase in probability, while in `BMI` it is only 15%.

*Comparing the coefficient estimates when adding a predictor*

The coefficient for `Glucose` changes from 0.379 in the original model to 0.352.
This is because people who have high `Glucose` are likely to have high `BMI`.
The two factors have a small positive correlation (0.221), as indicated
in the correlation matrix from before.

*Adding additional input variables*

We check if `Pregnancies` is a significant factor.

```{r}
fit.3 <- glm(Outcome ~ Glucose + BMI + Pregnancies, diabetes,
             family = binomial(link="logit"))
display(fit.3, digits = 4)
```

Number of pregnancies has small standard error and is statistically significant from 0 
at 95% interval (0.0835, 0.1907). It is not obvious why high number of 
pregnancies is a risk factor.

Now we add DPF (hereditory factor). Although diabetes mellitus is not
genetic per se, DNA may influence the risk of developing it. This type of
diabetes tends to run in families.

```{r}
fit.4 <- glm(Outcome ~ Glucose + BMI + Pregnancies + DPF, 
             diabetes, family = binomial(link="logit"))
display(fit.4, digits = 4)
```

DPF has a positive coefficient as expected, but has substantial standard error.
At 95% interval of (0.3179, 1.4847) it is still quite statistically 
significant from 0.

`Age` is not known to be a risk factor for this type of diabetes. We shall
include `Age` and check our assumptions.

```{r}
fit.5 <- glm(Outcome ~ Glucose + BMI + Pregnancies + 
               DPF + Age, 
             diabetes, family = binomial(link="logit"))
display(fit.5, digits = 4)
```

`Age` is not statistically significant, since it has a large 
standard error. However, it has the correct sign in the coefficient.
This input may not influence the predictive power of the model
very much but will not hurt either. So we decided to keep it.

Now, we add `BloodPressure` and check if it is significant.

```{r}
fit.5 <- glm(Outcome ~ Glucose + BMI + Pregnancies + 
               DPF + Age + BloodPressure, 
             diabetes, family = binomial(link="logit"))
display(fit.5, digits = 4)
```

`BloodPressure` is only marginally statistically significant from 0.
The standard error is large, and consequently, its 95% interval is
(-0.0237, -0.0033). However, it has -ve coefficient. It is not
clear is lower blood pressure favors high probability of 
diagnosing diabetes. It is known that diabetes damages arteries 
and makes them targets for hardening, called atherosclerosis. This
can cause high blood pressure. We would expect a +ve sign
for the coefficient. This is a dubious input, and we will
exclude it from the model.

Finally, we will check for the remaining inputs: `SkinThickness` 
and `Insulin`. Unfortunately both of them have high occurance of
errors (zeros) in the column. 

```{r}
fit.6 <- glm(Outcome ~ Glucose + BMI + Pregnancies + 
               DPF + Age + SkinThickness + Insulin,
             diabetes, family = binomial(link="logit"))
display(fit.6, digits = 4)
```

Both `SkinThickness` and `Insulin` suffer from large standard error
and are not statistically significant from 0. 

`SkinThickness` measures subcutaneal fat, and we would
expect the sign of the coefficient to be positive. Given the -ve
sign of the coefficient, large standard error, and occurance 
of zeros in the data, we exclude this variable from the model.

We also exclude `Insulin` because it has large standard error
and large number of observation errors.


*Check for interactions*

`Age` and number of `Pregnancies` seems to have a natural 
correlation, as
indicated by the correlation matrix. `Age` by itself is not 
statistically significant from 0 (at 95%). The interaction
`Age:Pregnancies`
is a candidate for inclusion in the model. 

A quick fit analysis of all possible two-factor 
interactions^[See Appendix B.]
reveals that interaction between `Glucose` and `DPF` is
statistically significant. 

Our model so far:

::: {.fullwidth}
```{r}
fit.8 <- glm(Outcome ~ Glucose + BMI + Pregnancies + 
               DPF + Age + Pregnancies:Age + DPF:Glucose,
             diabetes, family = binomial(link="logit"))
display(fit.8, digits = 4)
```
:::


*Interpretation of interactions*

`Pregnancies:Age` has a coefficient of -0.0087, and it is 
only mildly statistically
significant from zero. It's 95% interval is (-0.0143, -0.0031). The
sign of the coefficient is negative while signs of coefficents of
`Age` and `Pregnanacies` are both positive. This can be interpreted two
ways. For every additional pregnancy, the value of -0.0087 is 
subtracted from the coefficient of `Age`. Subtracting -0.0087 from
0.0455 (coefficient of `Age` alone) results in 19% reduction in
the effect produced by `Age`. As subjects have more pregnancies, effect
of age on diagnosis of diabetes decreases. Similarly, every
1 year of additional age also decreases the effec to pregnancy
on diagnosis of diabetes.

`Glucose:DPF` is mildly statistically significant with a coefficient
of -0.0234. Its 95%
interval is (-0.007, -0.0398). `DPF` is a measure of hereditory
factors contributing to diagnosis of diabetes. It is a ratio with
values in the range (0, 1). One way to interpret this is that, if
blood glucose increases in an individual by 10 mg/dL, then
effect of hereditory factors as represented by `DPF` decreases
by 0.234 (0.0234*10). Given that `DPF` has a coefficient of 3.947,
the level of reduction is about 6%.





```{r}
-0.0087   + 2* 0.0028
-0.0087   -2*0.0028
-0.0087/ 0.0455

-0.0234 +2*  0.0082
-0.0234  -2* 0.0082
diabetes$DPF
-0.234/ 3.947
```


_Evaluating, checking, and comparing fitted logistic regressions_


`SkinThickness`, Insulin

BloodPressure + 
    SkinThickness + Insulin


```{r}
0.1371-2*0.0268
0.9013 -2*0.2917
-0.0135-2*0.0051
-0.0135+2*0.0051
```




We add the remaining variables one by one and check for statistical significance.

Pregnancies + Glucose + BloodPressure + 
    SkinThickness + Insulin + BMI + DPF + Age,
    
    




```{r}

#display(fit.4)
#tidy(fit.4)
#glance(fit.4)
```





# Results

# Findings

# Appendix A

# Appendix B


```{r}
# fit.7 <- glm(Outcome ~ Glucose + BMI + Pregnancies + 
#                DPF + Age,
#              diabetes, family = binomial(link="logit"))
# add1(fit.7, ~ .^2, test = "Chisq")
# 
# lrtest()
```



URL embedded in Rmd


see if marginal effect makes sense, at means
predicted probability, at means

betas of (mean - val) vs val

best fit, best practices, even though predictability is independednt.

you can see what % are correctly predicted by substituting values



So what’s the bottom line? In general, it might be best to use AIC and BIC together in model selection. For example, in selecting the number of latent classes in a model, if BIC points to a three-class model and AIC points to a five-class model, it makes sense to select from models with 3, 4 and 5 latent classes. AIC is better in situations when a false negative finding would be considered more misleading than a false positive, and BIC is better in situations where a false positive is as misleading as, or more misleading than, a false negative.
https://www.methodology.psu.edu/resources/AIC-vs-BIC/


The most important metrics are the Adjusted R-square, RMSE, AIC and the BIC. 
for linear 
Note that, these regression metrics are all internal measures, that is they have been computed on the same data that was used to build the regression model. They tell you how well the model fits to the data in hand, called training data set.

In general, we do not really care how well the method works on the training data. Rather, we are interested in the accuracy of the predictions that we obtain when we apply our method to previously unseen test data.

https://stackoverflow.com/questions/48877475/how-can-i-add-stars-to-broom-packages-tidy-function-output




